{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.9/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from parkour_env import parkour_env\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.9/site-packages/minerl/herobraine/hero/spaces.py:484: UserWarning: The Text MineRLSpace class is not fully implemented. This may cause problems when sampling an action of this type (even when getting a noop).\n",
      "  warnings.warn(\"The Text MineRLSpace class is not fully implemented. This may cause problems when sampling an action of this type (even when getting a noop).\")\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "FAST = True\n",
    "MPS = True\n",
    "RESOLUTION = (64, 64)\n",
    "env = parkour_env(resolution=RESOLUTION, map=\"bridge_hybrid2\", debug=DEBUG, fast=FAST, action_set=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_state = env.observation_space.sample()\n",
    "sample_action = env.action_space.sample()\n",
    "image_shape = sample_state['pov'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample action: 4\n",
      "Sample pov space (64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample action:\", sample_action)\n",
    "print(\"Sample pov space\", image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.distributions as Categorical\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if (getattr(torch, 'has_mps', False) and MPS) else 'cpu'\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    import ipywidgets as widgets\n",
    "plt.ion()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, image_shape, n_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # actor\n",
    "        self.actor = nn.Sequential(\n",
    "                        #nn.BatchNorm2d(3),\n",
    "                        nn.Conv2d(3*4, 32, kernel_size=3, stride=2),\n",
    "                        # nn.ReLU(),\n",
    "                        nn.Tanh(),\n",
    "                        nn.MaxPool2d(3, 1),\n",
    "                        #nn.BatchNorm2d(16),\n",
    "                        nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
    "                        # nn.ReLU(),\n",
    "                        nn.Tanh(),\n",
    "                        nn.MaxPool2d(3, 1),\n",
    "                        #nn.BatchNorm2d(32),\n",
    "                        nn.Conv2d(64, 64, kernel_size=3, stride=2),\n",
    "                        # nn.ReLU(),\n",
    "                        nn.Tanh(),\n",
    "                        nn.MaxPool2d(3, 1),\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(576, 512),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(512, n_actions),\n",
    "                        # nn.Softmax(dim=-1)\n",
    "                        nn.Softmax(dim=-1)\n",
    "                    )\n",
    "\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                        #nn.BatchNorm2d(3),\n",
    "                        nn.Conv2d(3*4, 32, kernel_size=3, stride=2),\n",
    "                        # nn.ReLU(),\n",
    "                        nn.Tanh(),\n",
    "                        nn.MaxPool2d(3, 1),\n",
    "                        #nn.BatchNorm2d(16),\n",
    "                        nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
    "                        # nn.ReLU(),\n",
    "                        nn.Tanh(),\n",
    "                        nn.MaxPool2d(3, 1),\n",
    "                        #nn.BatchNorm2d(32),\n",
    "                        nn.Conv2d(64, 64, kernel_size=3, stride=2),\n",
    "                        # nn.ReLU(),\n",
    "                        nn.Tanh(),\n",
    "                        nn.MaxPool2d(3, 1),\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(576, 512),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(512, 1)\n",
    "                    )\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "    def act(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        \n",
    "        return action.detach(), action_logprob.detach()\n",
    "    \n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.actor(state)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, image_shape, n_actions, lr_actor, lr_critic, gamma, K_epochs, eps_clip):\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(image_shape, n_actions).to(device)\n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(image_shape, n_actions).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            #state = torch.FloatTensor(state).to(device)\n",
    "            action, action_logprob = self.policy_old.act(state)\n",
    "        \n",
    "        self.buffer.states.append(state)\n",
    "        self.buffer.actions.append(action)\n",
    "        self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "        \n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            advantages = rewards - state_values.detach()   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    \n",
    "    def save(self, model_path):\n",
    "        torch.save(self.policy_old.state_dict(), model_path)\n",
    "   \n",
    "\n",
    "    def load(self, model_path):\n",
    "        try:\n",
    "            self.policy_old.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))\n",
    "            self.policy.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))\n",
    "        except:\n",
    "            'Failed Loading...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_state(obs):\n",
    "    x = torch.tensor(obs['pov'].copy(), dtype=torch.float32)\n",
    "    #x = torch.permute(x, (2,0,1)).unsqueeze(0) / 255\n",
    "    x = torch.permute(x, (2,0,1)) / 255  # for queue\n",
    "    return x\n",
    "class queue():\n",
    "    def __init__(self, n_stacked_img, dim, stride=1):\n",
    "        self.data = []\n",
    "        self.stride = stride\n",
    "        self.size = (n_stacked_img - 1) * stride + 1\n",
    "        for i in range(self.size):\n",
    "            self.data.append(torch.zeros(dim))\n",
    "    def push(self, img):\n",
    "        self.data[:-1] = self.data[1:]\n",
    "        self.data[-1] = img\n",
    "    def get(self):\n",
    "        return torch.cat(self.data[::self.stride]).unsqueeze(0).to(device)\n",
    "    def fill(self, img):\n",
    "        for i in range(self.size):\n",
    "            self.data[i] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_ep_len = 400                    # max timesteps in one episode\n",
    "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "save_model_freq = int(100)          # save model frequency (in num timesteps)\n",
    "model_path = 'ppo_adamw2.pt'\n",
    "\n",
    "update_timestep = max_ep_len * 4    # update policy every n timesteps\n",
    "K_epochs = 40                       # update policy for K epochs\n",
    "eps_clip = 0.2                      # clip parameter for PPO\n",
    "gamma = 0.99                        # discount factor\n",
    "\n",
    "lr_actor = 0.0003                   # learning rate for actor network\n",
    "lr_critic = 0.001                   # learning rate for critic network\n",
    "\n",
    "random_seed = 0                     # set random seed if required (0 = no random seed)\n",
    "\n",
    "n_actions = env.n_actions\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(image_shape, n_actions, lr_actor, lr_critic, gamma, K_epochs, eps_clip)\n",
    "#ppo_agent.load(model_path)\n",
    "success_video = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a89b42833140d0aa87e2e3ccd7000a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06374dacd19414898dd17e69e5e9569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.9/site-packages/minerl/herobraine/hero/spaces.py:484: UserWarning: The Text MineRLSpace class is not fully implemented. This may cause problems when sampling an action of this type (even when getting a noop).\n",
      "  warnings.warn(\"The Text MineRLSpace class is not fully implemented. This may cause problems when sampling an action of this type (even when getting a noop).\")\n",
      "/home/user/.local/lib/python3.9/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n",
      "/opt/conda/lib/python3.9/runpy.py:127: RuntimeWarning: 'minerl.utils.process_watcher' found in sys.modules after import of package 'minerl.utils', but prior to execution of 'minerl.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting environment.\n",
      "Resetting environment.\n",
      "1253 success\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "1644 success\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "3401 success\n",
      "3431 success\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "3828 success\n",
      "3866 success\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "4188 success\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "4416 success\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "4534 success\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "4770 success\n",
      "4805 success\n",
      "Resetting environment.\n",
      "4841 success\n",
      "4843 success\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "5216 success\n",
      "5220 success\n",
      "5262 success\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n",
      "5561 success\n",
      "5579 success\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "renderer = widgets.Output()\n",
    "display.display(renderer)\n",
    "plotter = widgets.Output()\n",
    "display.display(plotter)\n",
    "\n",
    "num_episodes = 10000\n",
    "total_time = 0\n",
    "time_step = 0\n",
    "reward_list = []\n",
    "que = queue(n_stacked_img=4, dim=(3, RESOLUTION[0], RESOLUTION[1]), stride=4)\n",
    "for i_episode in range(num_episodes):\n",
    "    start = time.time()\n",
    "    obs = env.reset()\n",
    "\n",
    "    best_reward = -100\n",
    "    total_reward = 0\n",
    "    reward = 0\n",
    "    tmp = extract_state(obs)\n",
    "    que.fill(tmp)\n",
    "    history = []\n",
    "    for t in range(max_ep_len):\n",
    "        env.render()\n",
    "        if t % 10 == 0 and FAST and is_ipython:\n",
    "            with renderer:\n",
    "                display.clear_output(wait=True)\n",
    "                plt.imshow( obs['pov'] )\n",
    "                plt.title(f'{i_episode} step: {t} tot_reward: {total_reward:.2f} time: {total_time/60:.2f} min')\n",
    "                plt.show()\n",
    "        \n",
    "        # select action with policy\n",
    "        state = extract_state(obs)\n",
    "        que.push(state)\n",
    "        action = ppo_agent.select_action(que.get())\n",
    "        obs, reward, done, info, success = env.step(action)\n",
    "        total_reward += reward\n",
    "        history.append((state, action, reward))\n",
    "        best_reward = max(reward, best_reward)\n",
    "\n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "        time_step += 1\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # save model weights\n",
    "        if time_step % save_model_freq == 0 and not DEBUG:\n",
    "            ppo_agent.save(model_path)\n",
    "\n",
    "        if done:\n",
    "            reward_list.append(total_reward)\n",
    "            if is_ipython:\n",
    "                with plotter:\n",
    "                    display.clear_output(wait=True)\n",
    "                    plt.scatter(range(len(reward_list)), reward_list)\n",
    "                    plt.show()\n",
    "            break\n",
    "\n",
    "    end = time.time()\n",
    "    end = end - start\n",
    "    total_time += end\n",
    "    if success:\n",
    "        print(f\"{i_episode} success\")\n",
    "        success_video.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(reward_list)), reward_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython import display\n",
    "len(success_video)\n",
    "play = -1\n",
    "# Create a plot\n",
    "fig, ax = plt.subplots(1,1)\n",
    "t = 0\n",
    "A = ['forward', 'forward_jump', 'forward_sprint', 'camera_left', 'camera_right']\n",
    "ims = []\n",
    "def update(i):\n",
    "    state, action, reward = success_video[play][i]\n",
    "    title = ax.set_title(f'step: {i+1} reward: {float(reward):.2f} action: {A[action]}')\n",
    "    img = torch.permute(state, (1,2,0))*255\n",
    "    img = np.array(img.numpy(), dtype=int)\n",
    "    im = ax.imshow(img)\n",
    "    return im,\n",
    "anim = animation.FuncAnimation(fig, update, frames=len(success_video[play]), interval=50, repeat=True)\n",
    "anim.save(\"PPO_jump_newrwd.mp4\", dpi=80)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    ppo_agent.save(model_path)\n",
    "print(len(success_video))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4f7e2a837d202febde7ca43873e66ef03435144f2808f9a8051bdb7fbfc88b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
